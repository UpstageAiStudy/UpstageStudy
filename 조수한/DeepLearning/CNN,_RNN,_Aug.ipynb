{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbl7FjpgM5f5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import albumentations as A\n",
        "from torch.utils.data import Dataset\n",
        "from albumentations.pytorch import ToTensorV2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "  [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))]\n",
        ")\n",
        "\n",
        "batch_size = 32\n",
        "lr = 0.0001\n",
        "num_epoch = 10\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                      download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                      download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                        shuffle=True, num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-TRCE_rjUe5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc1 = nn.Linear(7 * 7 * 50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))  # 28x28x1 -> 28x28x20\n",
        "        x = self.pool(x)           # 28x28x20 -> 14x14x20\n",
        "        x = F.relu(self.conv2(x))  # 14x14x20 -> 14x14x50\n",
        "        x = self.pool(x)           # 14x14x50 -> 7x7x50\n",
        "        x = x.view(-1, 7 * 7 * 50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "dcIP3lx-hl2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1, padding=2)\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc1 = nn.Linear(7 * 7 * 50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))  # 28x28x1 -> 28x28x20\n",
        "        x = self.pool(x)           # 28x28x20 -> 14x14x20\n",
        "        x = F.relu(self.conv2(x))  # 14x14x20 -> 14x14x50\n",
        "        x = self.adaptive_pool(x)  # 14x14x50 -> 7x7x50\n",
        "        x = x.view(-1, 7 * 7 * 50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def weight_initialization(self, weight_init_method='xavier'):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if weight_init_method == 'gaussian':\n",
        "                    nn.init.normal_(m.weight)\n",
        "                elif weight_init_method == 'xavier':\n",
        "                    nn.init.xavier_normal_(m.weight)\n",
        "                elif weight_init_method == 'kaiming':\n",
        "                    nn.init.kaiming_normal_(m.weight)\n",
        "                elif weight_init_method == 'zeros':\n",
        "                    nn.init.zeros_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                if weight_init_method == 'gaussian':\n",
        "                    nn.init.normal_(m.weight)\n",
        "                elif weight_init_method == 'xavier':\n",
        "                    nn.init.xavier_normal_(m.weight)\n",
        "                elif weight_init_method == 'kaiming':\n",
        "                    nn.init.kaiming_normal_(m.weight)\n",
        "                elif weight_init_method == 'zeros':\n",
        "                    nn.init.zeros_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n"
      ],
      "metadata": {
        "id": "Lri6kExtK9UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CNN().to(device)\n",
        "model.weight_initialization('gaussian')\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "aHcaZMffLQ4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(num_epoch):\n",
        "  for index, (data, target) in enumerate(train_loader):\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if index % 100 == 0:\n",
        "          print(f\"loss of {epoch} epoch, {index} index : {loss.item()}\")"
      ],
      "metadata": {
        "id": "CaK9wxn3b0io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for data, target in test_loader:\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    output = model(data)\n",
        "    test_loss += criterion(output, target).item()\n",
        "    pred = output.argmax(dim=1, keepdim=True)\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "print('\\n 평균 loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "ddE9iVrCfdbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AlbumentationsMNIST\n",
        "\n",
        "https://albumentations-demo.herokuapp.com/"
      ],
      "metadata": {
        "id": "q1jpAbDb8fZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlbumentationsMNIST(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[idx]\n",
        "\n",
        "        image_np = np.array(image)\n",
        "\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image_np)\n",
        "            image_transformed = transformed['image']\n",
        "        else:\n",
        "            image_transformed = image_np\n",
        "\n",
        "        return image_transformed, label"
      ],
      "metadata": {
        "id": "-MarUcRD9OXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_transform = A.Compose([\n",
        "    A.Rotate(limit=20, p=0.5),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "trainset_augmented = AlbumentationsMNIST(trainset, transform=aug_transform)\n",
        "testset_augmented = AlbumentationsMNIST(testset, transform=aug_transform)"
      ],
      "metadata": {
        "id": "NG750VmdCBPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(trainset_augmented, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(testset_augmented, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "2j1aTKafCRrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "UljB405WGm1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
        "        self.gru = nn.GRU(input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(self.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, rnn):\n",
        "        if rnn == 'lstm':\n",
        "            rnn_layer = self.lstm\n",
        "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "            out, _ = rnn_layer(x, (h0, c0))\n",
        "        else:\n",
        "            rnn_layer = self.gru\n",
        "            h = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "            out, _ = rnn_layer(x, h)\n",
        "\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "    def weight_initialization(self, weight_init_method='xavier'):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                if weight_init_method == 'gaussian':\n",
        "                    nn.init.normal_(m.weight)\n",
        "                elif weight_init_method == 'xavier':\n",
        "                    nn.init.xavier_normal_(m.weight)\n",
        "                elif weight_init_method == 'kaiming':\n",
        "                    nn.init.kaiming_normal_(m.weight)\n",
        "                elif weight_init_method == 'zeros':\n",
        "                    nn.init.zeros_(m.weight)\n",
        "\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "            elif isinstance(m, nn.LSTM) or isinstance(m, nn.GRU):\n",
        "                for name, param in m.named_parameters():\n",
        "                    if 'weight_ih' in name:\n",
        "                        if weight_init_method == 'gaussian':\n",
        "                            nn.init.normal_(param)\n",
        "                        elif weight_init_method == 'xavier':\n",
        "                            nn.init.xavier_normal_(param)\n",
        "                        elif weight_init_method == 'kaiming':\n",
        "                            nn.init.kaiming_normal_(param)\n",
        "                        elif weight_init_method == 'zeros':\n",
        "                            nn.init.zeros_(param)\n",
        "                    elif 'weight_hh' in name:\n",
        "                        if weight_init_method == 'gaussian':\n",
        "                            nn.init.normal_(param)\n",
        "                        elif weight_init_method == 'xavier':\n",
        "                            nn.init.xavier_normal_(param)\n",
        "                        elif weight_init_method == 'kaiming':\n",
        "                            nn.init.kaiming_normal_(param)\n",
        "                        elif weight_init_method == 'zeros':\n",
        "                            nn.init.zeros_(param)\n",
        "                    elif 'bias' in name:\n",
        "                        nn.init.zeros_(param)\n"
      ],
      "metadata": {
        "id": "DFxHJ3MTGoHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 28\n",
        "input_size = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "num_epochs = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)"
      ],
      "metadata": {
        "id": "mRK8eVLhHMM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in model.parameters():\n",
        "    print(p.size())"
      ],
      "metadata": {
        "id": "KARk4CAzHP6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "model.weight_initialization('gaussian')\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "jq6CChaVHQh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(num_epoch):\n",
        "  for index, (data, target) in enumerate(train_loader):\n",
        "      data, target = data.reshape(-1, sequence_length, input_size).to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data, 'lstm')\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if index % 100 == 0:\n",
        "          print(f\"loss of {epoch} epoch, {index} index : {loss.item()}\")"
      ],
      "metadata": {
        "id": "OXkRziWIHozx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, (data, target) in enumerate(train_loader):\n",
        "      data, target = data.reshape(-1, sequence_length, input_size).to(device), target.to(device)\n",
        "      print(data, target)\n",
        "      print(data.shape)\n",
        "      break\n",
        "\n",
        "for index, (data, target) in enumerate(train_loader):\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      print(data, target)\n",
        "      print(data.shape)\n",
        "      break"
      ],
      "metadata": {
        "id": "9ZVc3Z64JpQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for data, target in test_loader:\n",
        "    data, target = data.reshape(-1, sequence_length, input_size).to(device), target.to(device)\n",
        "    output = model(data, 'lstm')\n",
        "    test_loss += criterion(output, target).item()\n",
        "    pred = output.argmax(dim=1, keepdim=True)\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "print('\\n 평균 loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "8otSIz0qI_0H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}